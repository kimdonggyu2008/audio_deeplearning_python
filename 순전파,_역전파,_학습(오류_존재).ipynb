{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNF9EAhN4/Ouvxt5zZ3b5/b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/audio_deeplearning_python/blob/main/%EC%88%9C%EC%A0%84%ED%8C%8C%2C_%EC%97%AD%EC%A0%84%ED%8C%8C%2C_%ED%95%99%EC%8A%B5(%EC%98%A4%EB%A5%98_%EC%A1%B4%EC%9E%AC).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASSnGiqAD7f_",
        "outputId": "1f5800e5-f5bb-423b-a44b-a8fe2979d531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 0.00037670635870269687 at epoch 1\n",
            "Error: 0.00037636591612477926 at epoch 2\n",
            "Error: 0.0003760558169522757 at epoch 3\n",
            "Error: 0.00037575164777837864 at epoch 4\n",
            "Error: 0.00037545229507613364 at epoch 5\n",
            "Error: 0.00037515700843509197 at epoch 6\n",
            "Error: 0.0003748652534021549 at epoch 7\n",
            "Error: 0.00037457662862947183 at epoch 8\n",
            "Error: 0.0003742908214488524 at epoch 9\n",
            "Error: 0.00037400758233048633 at epoch 10\n",
            "Error: 0.0003737267089998846 at epoch 11\n",
            "Error: 0.00037344803577949316 at epoch 12\n",
            "Error: 0.0003731714259634921 at epoch 13\n",
            "Error: 0.0003728967660960643 at epoch 14\n",
            "Error: 0.0003726239615360078 at epoch 15\n",
            "Error: 0.0003723529329457908 at epoch 16\n",
            "Error: 0.0003720836134758501 at epoch 17\n",
            "Error: 0.0003718159464882503 at epoch 18\n",
            "Error: 0.0003715498837072351 at epoch 19\n",
            "Error: 0.00037128538371199177 at epoch 20\n",
            "Error: 0.0003710224107059064 at epoch 21\n",
            "Error: 0.00037076093351035943 at epoch 22\n",
            "Error: 0.0003705009247414326 at epoch 23\n",
            "Error: 0.0003702423601359689 at epoch 24\n",
            "Error: 0.00036998521799971363 at epoch 25\n",
            "Error: 0.00036972947875537574 at epoch 26\n",
            "Error: 0.00036947512457248874 at epoch 27\n",
            "Error: 0.0003692221390642929 at epoch 28\n",
            "Error: 0.0003689705070395341 at epoch 29\n",
            "Error: 0.0003687202142992449 at epoch 30\n",
            "Error: 0.0003684712474704134 at epoch 31\n",
            "Error: 0.0003682235938698568 at epoch 32\n",
            "Error: 0.00036797724139284367 at epoch 33\n",
            "Error: 0.0003677321784219989 at epoch 34\n",
            "Error: 0.0003674883937527983 at epoch 35\n",
            "Error: 0.0003672458765326569 at epoch 36\n",
            "Error: 0.00036700461621113375 at epoch 37\n",
            "Error: 0.00036676460249922537 at epoch 38\n",
            "Error: 0.00036652582533609735 at epoch 39\n",
            "Error: 0.00036628827486188064 at epoch 40\n",
            "Error: 0.00036605194139542987 at epoch 41\n",
            "Error: 0.0003658168154161254 at epoch 42\n",
            "Error: 0.00036558288754897234 at epoch 43\n",
            "Error: 0.0003653501485523898 at epoch 44\n",
            "Error: 0.0003651185893081787 at epoch 45\n",
            "Error: 0.0003648882008132835 at epoch 46\n",
            "Error: 0.0003646589741729801 at epoch 47\n",
            "Error: 0.00036443090059525745 at epoch 48\n",
            "Error: 0.00036420397138613636 at epoch 49\n",
            "Error: 0.00036397817794576494 at epoch 50\n",
            "Training complete!\n",
            "=====\n",
            "\n",
            "Our network believes that 0.3 + 0.1 is equal to 0.3938863505612629\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from random import random\n",
        "\n",
        "#더 많은 층수를 가지는 mlp?\n",
        "class MLP(object):\n",
        "#3개 입력을 받고 2개의 출력을 시행함\n",
        "  def __init__(self,num_inputs=3,hidden_layers=[3,3], num_outputs=2):\n",
        "    #입력 갯수, 레이어 갯수, 출력 데이터 갯수 지정\n",
        "\n",
        "    self.num_inputs=num_inputs\n",
        "    self.hidden_layers=hidden_layers# 첫번째 층이 3개 뉴런, 두번째 층이 3개 뉴런을 가지게 만듦\n",
        "    self.num_outputs=num_outputs\n",
        "\n",
        "    layers = [num_inputs] + hidden_layers + [num_outputs]\n",
        "\n",
        "    weights=[] #랜덤한 가중치용 배열 생성\n",
        "    for i in range(len(layers)-1):\n",
        "      w=np.random.rand(layers[i],layers[i+1])#i번째 레이어 -> i+1번째 레이어로 넘어가는 과정의 가중치 갯수 세기\n",
        "      weights.append(w)# 다음 층으로 넘어갈때 각 레이어별 갯수가 맞아야 함\n",
        "    self.weights=weights #현재 층에서의 가중치\n",
        "\n",
        "#각 레이어 간 기울기 배열 생성(레이어 사이이므로 레이어 갯수 -1이 되어야 함)\n",
        "    derivatives=[]\n",
        "    for i in range(len(layers)-1):#층 간 가중치의 도함수를 계산\n",
        "      d=np.zeros((layers[i],layers[i+1]))#앞 출력값의 갯수, 뒷 입력값의 갯수 크기만큼의 도함수 행렬 생성\n",
        "      derivatives.append(d)#해당 레이어 -> 다음 레이어로 넘어가는 과정에서, 각 가중치에 대한 그레디언트 행렬\n",
        "    self.derivatives=derivatives #현재 층->다음 층에서의 그레디언트\n",
        "      #최종적으로는 각 층간 가중치 배열을 가지게 됨. 2개 레이어면 1개의 가중치 배열만 가지게 됨\n",
        "\n",
        "#각 레이어의 활성화 값 지정\n",
        "\n",
        "    activations=[]\n",
        "    for i in range(len(layers)): #각 층에 대한 활성화 값\n",
        "      a=np.zeros(layers[i])#해당 층이 가지는 가중치 크기만큼의  zeros배열\n",
        "      activations.append(a)#추가\n",
        "    self.activations=activations#현재 층에서의 활성화 값\n",
        "    #각 층의 최종 활성화 값들을 가지는 배열이 됨\n",
        "\n",
        "\n",
        "  #순전파 학습\n",
        "  def forward_propagate(self,inputs):#시그모이드가 적용된 input값 사용\n",
        "    activations=inputs#활성화 함수\n",
        "    self.activations[0]=inputs\n",
        "    for i, w in enumerate(self.weights):# 현재 층에서의 가중치를 가져옴\n",
        "      net_inputs=np.dot(activations,w)#현재 층의 input값과 현재 층의 가중치 행렬곱 , act 2층 * weight 2층\n",
        "      activations=self._sigmoid(net_inputs)#주어진 행렬곱의 시그모이드로 구별시킴, 시그모이드(net_input(2층의 결괏값))\n",
        "      self.activations[i+1]=activations #시그모이드가 적용된 행렬곱을 저장, act 3층 = 2층의 시그모이드(net_input(2층의 결괏값))\n",
        "      #예시) 3층의 결괏값 = activation 2층 * weight 2층\n",
        "      # activation 3층 = 시그모이드(3층의 결괏값)\n",
        "\n",
        "    return activations\n",
        "\n",
        "\n",
        "\n",
        "#역전파는 f(x) * (1-f(x))의 형태로, 실제값-최종 결과의 지분을 각 가중치에 따라 나눈다고 생각하면 될듯\n",
        "#해당 결과가 나오기 위해 통한 경로의 오류의 가중치 값들은 전부 곱하면 됨(결과 -> 결과-1층 -> .... 현재층까지)\n",
        "  def back_propagate(self,error):\n",
        "    for i in reversed(range(len(self.derivatives))):#뒤쪽에서부터 돌아옴, 역전파는 레이어 사이에서 벌어지므로 레이어-1번 반복(=derivates 길이)\n",
        "      activations = self.activations[i+1]#현재 위치의 활성화 함수, 뒤에서부터 시작됨\n",
        "      delta = error * self._sigmoid_derivative(activations)#역 시그모이드 f(x)*(1-f(x))\n",
        "      delta_reshaped = delta.reshape(delta.shape[0], -1).T #역 시그모이드를 통해 바뀐 행렬의 모양변경\n",
        "      current_activations = self.activations[i]#해당 층의 가중치 배열을 가져옴\n",
        "      current_activations_reshaped = current_activations.reshape(current_activations.shape[0],-1)#해당 가중치 배열을 반대로 변경\n",
        "      print(\"현재 activation 크기는\",current_activations_reshaped.shape)\n",
        "      print(\"현재 델타 크기는\",delta_reshaped.shape)\n",
        "\n",
        "      self.derivatives[i] = np.dot(current_activations_reshaped, delta_reshaped)\n",
        "      error=np.dot(delta,self.weights[i].T)\n",
        "  #에러는 최종 결과값들 에러의 총합, 가중치 업데이트에는 합쳐지기 전 각 에러값들에 대한 결괏값이 필요함\n",
        "  #에러의 총합 -> 합쳐지기 전 에러값 -> 적용된 시그모이드를 역함수로 다시 되돌림 -> 해당 에러가 현재 레이어에서 얼마나 영향을 받았나, 현재 에러의 지분\n",
        "\n",
        "\n",
        "  def train(self,inputs,targets,epochs,learning_rate):\n",
        "    for i in range(epochs):\n",
        "      sum_error=0\n",
        "      for j,input in enumerate(inputs):\n",
        "        target=targets[j]\n",
        "        output=self.forward_propagate(inputs)\n",
        "        error=target-output\n",
        "        self.back_propagate(error)\n",
        "        self.gradient_descent(learning_rate)\n",
        "        sum_error+=self.mse(target,output)\n",
        "      print(\"Error: {} at epoch {}\".format(sum_errors / len(items), i+1))\n",
        "    print(\"트레이닝 완료\")\n",
        "\n",
        "  def gradient_descent(self,learning_rate=1):\n",
        "    for i in range(len(self.weights)):#해당 레이어의 가중치 갯수(뉴런 갯수)만큼 반복\n",
        "      weights=self.weights[i]\n",
        "      derivatives=self.derivatives[i]\n",
        "      weight+=derivatives*learning_rate\n",
        "\n",
        "  def _sigmoid(self,x):#시그모이드 함수\n",
        "    y=1.0/(1+np.exp(-x))\n",
        "    return y\n",
        "\n",
        "  def _sigmoid_derivative(self,x):#역 시그모이드 함수\n",
        "    return x*(1.0-x)\n",
        "\n",
        "  def _mse(self,target,output):#평균제곱오차\n",
        "    return np.average((target-output)**2)\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "\n",
        "  items = np.array([[random()/2 for _ in range(2)] for _ in range(1000)])\n",
        "  targets = np.array([[i[0] + i[1]] for i in items])\n",
        "\n",
        "  mlp.train(items, targets, 50, 0.1)\n",
        "\n",
        "    # create dummy data\n",
        "  input = np.array([0.3, 0.1])\n",
        "  target = np.array([0.4])\n",
        "\n",
        "    # get a prediction\n",
        "  output = mlp.forward_propagate(input)\n",
        "\n",
        "  print()\n",
        "  print(\"Our network believes that {} + {} is equal to {}\".format(input[0], input[1], output[0]))\n"
      ]
    }
  ]
}