{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8Rp7fQjx0zgoWtjhzFKSn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/audio_deeplearning_python/blob/main/cnn_%EC%9D%8C%EC%95%85%EC%9E%A5%EB%A5%B4_%EB%B6%84%EB%A5%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNf4xSBP3yc-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HODPBhH3E8_9",
        "outputId": "5f725b03-cc20-424d-f45d-e7d8cbfbaf76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH=\"/content/drive/MyDrive/오디오_딥러닝_파이썬/result/data_10.json\"\n"
      ],
      "metadata": {
        "id": "HrjYvA7zFG91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_path):\n",
        "  with open(data_path,\"r\") as fp:#json파일 읽어옴\n",
        "    data=json.load(fp)\n",
        "\n",
        "  X=np.array(data[\"mfcc\"])#각 라벨별 데이터 가져와서 x,y에 넣음\n",
        "  y=np.array(data[\"labels\"])\n",
        "  return X,y\n",
        "\n"
      ],
      "metadata": {
        "id": "c8kQ8xsp4FDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):#학습 경과 그래프 그리기\n",
        "  fig,axs=plt.subplot(2)\n",
        "\n",
        "  axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "  axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "  axs[0].set_ylabel(\"Accuracy\")\n",
        "  axs[0].legend(loc=\"lower right\")\n",
        "  axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # create error sublpot\n",
        "  axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "  axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "  axs[1].set_ylabel(\"Error\")\n",
        "  axs[1].set_xlabel(\"Epoch\")\n",
        "  axs[1].legend(loc=\"upper right\")\n",
        "  axs[1].set_title(\"Error eval\")\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "zGXZu9x04cO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_datasets(test_size,validation_size):\n",
        "  X,y=load_data(DATA_PATH)#데이터 읽어오기\n",
        "\n",
        "  X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=test_size)\n",
        "  #학습셋, 테스트셋 사이즈 크기로 분리\n",
        "  X_train,X_validation,y_train,y_validation=train_test_split(X_train,y_train,test_size=validation_size)\n",
        "  #이미 나눠진 트레인 셋에서 평가셋 분리, mfcc값을 학습셋, 테스트셋으로 분리함\n",
        "  X_train=X_train[...,np.newaxis]\n",
        "  # 학습용 mfcc값에 축 추가\n",
        "  X_validation=X_validation[...,np.newaxis]\n",
        "  # 평가용 mfcc값에 축 추가\n",
        "  X_test=X_test[...,np.newaxis]\n",
        "  # 테스트용 mfcc값에 축 추가\n",
        "\n",
        "  return X_train,X_validation,X_test,y_train,y_validation,y_test"
      ],
      "metadata": {
        "id": "ZJut8LG145uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape):\n",
        "  #input_shape는 데이터에 따라서 달라짐, 이미지 100*100은 100*100*rgb(3)\n",
        "  model=keras.Sequential()\n",
        "\n",
        "  model.add(keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=input_shape))\n",
        "  # 출력 채널 32개, 커널크기 3*3\n",
        "  model.add(keras.layers.MaxPooling2D((3,3),strides=(2,2),padding='same'))\n",
        "  # 풀링 층 추가, padding = same는 주변 패딩 추가해서 크기 유지\n",
        "  model.add(keras.layers.BatchNormalization())# 해당 데이터 값 정규화(퀀타이징)\n",
        "\n",
        "  model.add(keras.layers.Conv2D(32,(3,3),activation='relu'))\n",
        "  model.add(keras.layers.MaxPooling2D((3,3),strides=(2,2),padding='same'))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(keras.layers.Conv2D(32,(2,2),activation='relu'))\n",
        "  #필터크기 2*2로 변경해서 작게 만듦\n",
        "  model.add(keras.layers.MaxPooling2D((3,3),strides=(2,2),padding='same'))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(keras.layers.Flatten())\n",
        "  #차원 줄임\n",
        "  model.add(keras.layers.Dense(64,activation='relu'))\n",
        "  model.add(keras.layers.Dropout(0.3))\n",
        "  #드롭아웃\n",
        "  model.add(keras.layers.Dense(10,activation='softmax'))\n",
        "  #소프트맥스 활성화함수 지정\n",
        "  return model"
      ],
      "metadata": {
        "id": "V8_oeYdG5kMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model,X,y):\n",
        "  X=X[np.newaxis,...]\n",
        "\n",
        "  prediction=model.predict(X)\n",
        "\n",
        "  predicted_index=np.argmax(prediction,axis=1)\n",
        "  print(\"Target: {}, Predicted label: {}\".format(y, predicted_index))"
      ],
      "metadata": {
        "id": "66rdH0xx6Wn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "  #학습셋, 평가셋 생성\n",
        "  X_train,X_validation,X_test,y_train,y_validation,y_test=prepare_datasets(0.25,0.2)#테스트셋 비율, 평가셋 비율\n",
        "  input_shape=(X_train.shape[1],X_train.shape[2],1)#학습에 사용한 값 지정, mfcc값?\n",
        "  model=build_model(input_shape)\n",
        "  #모델 생성\n",
        "  optimiser=keras.optimizers.Adam(learning_rate=0.0001)\n",
        "  #\n",
        "  model.compile(optimizer=optimiser,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "  history=model.fit(X_train,y_train,validation_data=(X_validation,y_validation),batch_size=32,epochs=30)\n",
        "  #최적화 과정\n",
        "  plot_history(history)\n",
        "\n",
        "  test_loss,test_acc=model.evaluate(X_test,y_test,verbose=2)\n",
        "  #차이값, 정확도 평가\n",
        "  print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "  X_to_predict=X_test[100]\n",
        "  y_to_predict=y_test[100]\n",
        "\n",
        "  predict(model,X_to_predict,y_to_predict)\n",
        "  #예측"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "olVsEs9p6xh5",
        "outputId": "8e0abf7f-d018-41d3-fc9f-738f3ac2e9f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8ffa83a730e1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-d392435f392e>\u001b[0m in \u001b[0;36mprepare_datasets\u001b[0;34m(test_size, validation_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zQUFPaiT7t-5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}